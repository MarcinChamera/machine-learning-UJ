{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa07391a",
   "metadata": {},
   "source": [
    "jakie są i jak używamy w modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e6d844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:19.112616Z",
     "start_time": "2022-05-12T13:44:13.107830Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "import scipy.special\n",
    "import sklearn.datasets as ds\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDRegressor, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras.losses\n",
    "import keras.backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96a3769",
   "metadata": {},
   "source": [
    "W bibliotece scikit-learn funkcje straty, które używane są także jako metryki, można znaleźć w sekcji \"sklearn.metrics: Metrics\" (https://scikit-learn.org/stable/modules/classes.html?highlight=metrics#module-sklearn.metrics)\n",
    "\n",
    "Jeśli chodzi o ich wykorzystanie w modelach, często parametr nazwany \"loss\" odpowiada za wybór funkcji straty - wówczas dostępnych funkcji straty należy szukać w dokumentacji konkretnego modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e75383",
   "metadata": {},
   "source": [
    "### dla problemu regresji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1073ed15",
   "metadata": {},
   "source": [
    "##### - jakie są\n",
    "\n",
    "poniżej przedstawiono dostępne implementacje funkcji straty, niestety w formie tak jakby pełniły rolę metryk, porównujących predykcję z rzeczywistymi wartościami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150e0738",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.607720Z",
     "start_time": "2022-05-12T13:44:19.114813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error: 0.06999999999999999\n",
      "mean_squared_error: 0.005299999999999998\n",
      "root_mean_squared_error: 0.07280109889280517\n",
      "mean_squared_log_error: 0.003385794800683798\n",
      "median_absolute_error: 0.06999999999999999\n",
      "mean_absolute_percentage_error: 0.30714285714285716\n",
      "mean_poisson_deviance: 0.025894618532841557\n",
      "mean_gamma_deviance: 0.16262512596243095\n",
      "mean_tweedie_deviance: 0.005299999999999998\n",
      "mean_pinball_loss: 0.034999999999999996\n",
      "cosine_similarity: [[1. 1.]\n",
      " [1. 1.]]\n",
      "huber_loss: [[0.0338 ]\n",
      " [0.00405]]\n",
      "log_cosh: [0.004044542376182347, 0.0012494776089713167]\n"
     ]
    }
   ],
   "source": [
    "dummy_y_true = np.array([[0.35], [0.14]])\n",
    "dummy_y_pred = np.array([[0.26], [0.09]])\n",
    "\n",
    "# z sklearn:\n",
    "mean_absolute_error = sklearn.metrics.mean_absolute_error(dummy_y_true, dummy_y_pred)\n",
    "mean_squared_error = sklearn.metrics.mean_squared_error(dummy_y_true, dummy_y_pred)\n",
    "root_mean_squared_error = sklearn.metrics.mean_squared_error(dummy_y_true, dummy_y_pred, squared=False)\n",
    "mean_squared_log_error = sklearn.metrics.mean_squared_log_error(dummy_y_true, dummy_y_pred)\n",
    "median_absolute_error = sklearn.metrics.median_absolute_error(dummy_y_true, dummy_y_pred)\n",
    "mean_absolute_percentage_error = sklearn.metrics.mean_absolute_percentage_error(dummy_y_true, dummy_y_pred)\n",
    "mean_poisson_deviance = sklearn.metrics.mean_poisson_deviance(dummy_y_true, dummy_y_pred)\n",
    "mean_gamma_deviance = sklearn.metrics.mean_gamma_deviance(dummy_y_true, dummy_y_pred)\n",
    "mean_tweedie_deviance = sklearn.metrics.mean_tweedie_deviance(dummy_y_true, dummy_y_pred)\n",
    "mean_pinball_loss = sklearn.metrics.mean_pinball_loss(dummy_y_true, dummy_y_pred)\n",
    "cosine_similarity = sklearn.metrics.pairwise.cosine_similarity(dummy_y_true, dummy_y_pred)\n",
    "\n",
    "# z scipy:\n",
    "huber_loss = scipy.special.huber(dummy_y_true, dummy_y_pred)\n",
    "\n",
    "# implementacja funkcji log_cosh występuje \"niestety\" tylko w bibliotece Keras, która nie jest w zakresie tego kursu,\n",
    "# natomiast jej działanie jak najbardziej można wykorzystać także i tutaj\n",
    "log_cosh = keras.losses.log_cosh(dummy_y_true, dummy_y_pred).numpy().tolist()\n",
    " \n",
    "print('mean_absolute_error:', mean_absolute_error)\n",
    "print('mean_squared_error:', mean_squared_error)\n",
    "print('root_mean_squared_error:', root_mean_squared_error)\n",
    "print('mean_squared_log_error:', mean_squared_log_error)\n",
    "print('median_absolute_error:', median_absolute_error)\n",
    "print('mean_absolute_percentage_error:', mean_absolute_percentage_error)\n",
    "print('mean_poisson_deviance:', mean_poisson_deviance)\n",
    "print('mean_gamma_deviance:', mean_gamma_deviance)\n",
    "print('mean_tweedie_deviance:', mean_tweedie_deviance)\n",
    "print('mean_pinball_loss:', mean_pinball_loss)\n",
    "print('cosine_similarity:', cosine_similarity)\n",
    "print('huber_loss:', huber_loss)\n",
    "print('log_cosh:', log_cosh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440541dc",
   "metadata": {},
   "source": [
    "funkcja straty MBE (Mean Bias Error) nie posiada implementacji w żadnej z przeszukanych bibliotek, ale nie jest trudno ją zaimplementować samemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5d46f4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.623721Z",
     "start_time": "2022-05-12T13:44:20.608721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_bias_error: -0.06999999999999999\n"
     ]
    }
   ],
   "source": [
    "def mean_bias_error(y_true, y_pred):\n",
    "    return np.mean(y_pred - y_true)\n",
    "\n",
    "print('mean_bias_error:', mean_bias_error(dummy_y_true, dummy_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35359fcf",
   "metadata": {},
   "source": [
    "##### - jak używamy w modelu\n",
    "\n",
    "W scikit-learn istnieją modele, np. dla regresji liniowej (sklearn.linear_model), gdzie funkcja straty jest już z góry wybrana. Dla przykładu:\n",
    "- LinearRegression to tak na prawdę tylko wrapper na \"least squares solver\" z biblioteki numpy, który wykorzystuje metodę Ordinary Least Squares\n",
    "- Ridge - \"This model solves a regression model where the loss function is the **linear least squares** function and regularization is given by the l2-norm\", minimalizuje funkcję:\n",
    "$$||y - X_w||^2_2 + alpha * ||w||^2_2$$\n",
    "- HuberRegressor oparty jest o funkcję straty Hubera\n",
    "- Istnieje także model SGDRegressor, który wykorzystuje Stochastic Gradient Descent, gdzie wagi są aktualizowane po każdej próbce. Dla tego modelu mamy cztery funkcje straty do wyboru: ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, ‘squared_epsilon_insensitive’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf18754b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.827735Z",
     "start_time": "2022-05-12T13:44:20.625722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[139.0, 72.0, 79.0, 280.0, 150.0]\n",
      "[182.97485282  84.32127311 109.46137067 218.79134389 109.20079235]\n"
     ]
    }
   ],
   "source": [
    "diabetes_ds = ds.load_diabetes()\n",
    "X = diabetes_ds.data\n",
    "y = diabetes_ds.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "sgd = SGDRegressor(loss='squared_error', max_iter=10000)\n",
    "sgd.fit(X_train, y_train)\n",
    "y_pred = sgd.predict(X_test)\n",
    "print(y_test.tolist()[:5])\n",
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6865ac1",
   "metadata": {},
   "source": [
    "### dla problemu klasyfikacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6cd960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T19:56:49.937799Z",
     "start_time": "2022-04-14T19:56:49.926800Z"
    }
   },
   "source": [
    "##### -jakie są"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a15177b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.843743Z",
     "start_time": "2022-05-12T13:44:20.829742Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brier_score_loss: 0.3333333333333333\n",
      "hamming_loss: 0.3333333333333333\n",
      "hinge_loss: 1.0\n",
      "log_loss: 0.21616187468057912\n",
      "zero_one_loss: 0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "dummy_binary_y_pred = [1, 1, 0]\n",
    "dummy_binary_y_true = [1, 0, 0]\n",
    "\n",
    "# dla dwóch klass\n",
    "brier_score_loss = sklearn.metrics.brier_score_loss(dummy_binary_y_true, dummy_binary_y_pred)\n",
    "# dla dwóch lub więcej klas\n",
    "hamming_loss = sklearn.metrics.hamming_loss(dummy_binary_y_true, dummy_binary_y_pred)\n",
    "# dla dwóch lub więcej klas\n",
    "hinge_loss = sklearn.metrics.hinge_loss(dummy_binary_y_true, dummy_binary_y_pred)\n",
    "# dla dwóch lub więcej klas\n",
    "zero_one_loss = sklearn.metrics.zero_one_loss(dummy_binary_y_true, dummy_binary_y_pred)\n",
    "\n",
    "dummy_y_true = ['spam', 'lettuce', 'lettuce', 'spam']\n",
    "dummy_onehot_y_pred = [[.1, .9], [.9, .1], [.8, .2], [.35, .65]]\n",
    "\n",
    "# aka cross-entropy loss - dla dwóch lub więcej klas\n",
    "log_loss = sklearn.metrics.log_loss(dummy_y_true, dummy_onehot_y_pred)\n",
    "\n",
    "print('brier_score_loss:', brier_score_loss)\n",
    "print('hamming_loss:', hamming_loss)\n",
    "print('hinge_loss:', hinge_loss)\n",
    "print('log_loss:', log_loss)\n",
    "print('zero_one_loss:', zero_one_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ef090",
   "metadata": {},
   "source": [
    "implementacje funkcji straty, które istnieją tylko w bibliotece Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a587e00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.891573Z",
     "start_time": "2022-05-12T13:44:20.844743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_hinge: 0.8654500246047974\n",
      "sparse_categorical_crossentropy: [0.05129344388842583, 2.3025851249694824]\n",
      "poisson: [0.35043105483055115, 1.1008614301681519]\n",
      "kl_divergence: [0.051292017102241516, 2.302582263946533]\n",
      "categorical_hinge: 0.0\n"
     ]
    }
   ],
   "source": [
    "dummy_binary_y_true = keras.backend.variable(np.asarray([1, 1, 0, 0]))\n",
    "dummy_binary_y_pred = keras.backend.variable(np.asarray([0.8, 0.65, 0.43, 0.12]))\n",
    "\n",
    "dummy_multi_onehot_y_true = keras.backend.variable(np.asarray([[0, 1, 0], [0, 0, 1]]))\n",
    "dummy_multi_y_pred = keras.backend.variable(np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]]))\n",
    "\n",
    "dummy_multi_y_true = keras.backend.variable(np.asarray([1, 2]))\n",
    "\n",
    "# dla dwóch klas\n",
    "squared_hinge = keras.losses.squared_hinge(dummy_binary_y_true, dummy_binary_y_pred)\n",
    "# dla dwóch lub więcej klas\n",
    "sparse_categorical_crossentropy = keras.losses.sparse_categorical_crossentropy(dummy_multi_y_true, dummy_multi_y_pred)\n",
    "# dla dwóch lub więcej klas\n",
    "poisson = keras.losses.poisson(dummy_multi_onehot_y_true, dummy_multi_y_pred)\n",
    "# dla dwóch lub więcej klas\n",
    "kl_divergence = keras.losses.kl_divergence(dummy_multi_onehot_y_true, dummy_multi_y_pred)\n",
    "# dla dwóch klas\n",
    "categorical_hinge = keras.losses.categorical_hinge(dummy_binary_y_true, dummy_binary_y_pred)\n",
    "\n",
    "print('squared_hinge:', squared_hinge.numpy().tolist())\n",
    "print('sparse_categorical_crossentropy:', sparse_categorical_crossentropy.numpy().tolist())\n",
    "print('poisson:', poisson.numpy().tolist())\n",
    "print('kl_divergence:', kl_divergence.numpy().tolist())\n",
    "print('categorical_hinge:', categorical_hinge.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196b7e6",
   "metadata": {},
   "source": [
    "##### -jak używamy w modelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb012204",
   "metadata": {},
   "source": [
    "jeśli chodzi o użycie funkcji straty w dostępnych modelach sklearn, przykłady:\n",
    "- model LinearSVC, czyli Linear Support Vector Classification, pozwala na wybranie jako funkcji straty jedną z dwóch: hinge lub squared hinge\n",
    "- model LogisticRegression w wypadku problemu wieloklasowego uzywa funkcji log loss, czyli wspomniana wcześniej cross-entropy\n",
    "- model SGDClassifier, czyli estymator wykorzystujący SGD, który implementuje różne modele liniowe poprzez kontrolę parametru loss - czyli wybranie odpowiedniej funkcji straty powoduje wybranie konkretnego modelu, np. wybranie funkcji hinge wybiera liniowy model SVM. Dostępne funkcje straty dla SGDClassifier: ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ed9cb0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.939578Z",
     "start_time": "2022-05-12T13:44:20.893581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 2, 2, 0, 0, 0, 0, 2, 2]\n",
      "[0 2 2 2 0 0 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = ds.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# categorical cross-entropy loss\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_test.tolist()[:10])\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a08fa57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-12T13:44:20.955579Z",
     "start_time": "2022-05-12T13:44:20.940581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 2, 2, 0, 0, 0, 0, 2, 2]\n",
      "[0 2 2 2 0 0 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# wieloklasowy SVM\n",
    "model = LinearSVC(loss='squared_hinge', dual=False, multi_class='ovr')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(y_test.tolist()[:10])\n",
    "print(y_pred[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
